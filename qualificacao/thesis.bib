%
% This is file `coppe.bib'.
%
% Bibliographic references for the documentation.
%
% Copyright (C) 2011 CoppeTeX Project and any individual authors listed
% elsewhere in this file.
%
% This program is free software; you can redistribute it and/or modify
% it under the terms of the GNU General Public License version 3 as
% published by the Free Software Foundation.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
% GNU General Public License version 3 for more details.
%
% You should have received a copy of the GNU General Public License
% version 3 along with this package (see COPYING file).
% If not, see <http://www.gnu.org/licenses/>.
%
% $URL: https://coppetex.svn.sourceforge.net/svnroot/coppetex/trunk/coppe.bib $
% $Id: coppe.bib 118 2008-10-18 14:17:06Z helano $
%
% Author(s): Vicente H. F. Batista
%            George O. Ainsworth Jr.
%

@article{atkinson2006-effective-pattern-matching,
    author = {Atkinson, Darren and Griswold, William},
    year = {2006},
    month = {04},
    pages = {413-447},
    title = {Effective pattern matching of source code using abstract syntax patterns},
    volume = {36},
    journal = {Softw., Pract. Exper.},
    doi = {10.1002/spe.704}
}

@inproceedings{premtoon2020-code-search-equational-reasoning,
    author = {Premtoon, Varot and Koppel, James and Solar-Lezama, Armando},
    title = {Semantic code search via equational reasoning},
    year = {2020},
    isbn = {9781450376136},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3385412.3386001},
    doi = {10.1145/3385412.3386001},
    abstract = {We present a new approach to semantic code search based on equational reasoning, and the Yogo tool implementing this approach. Our approach works by considering not only the dataflow graph of a function, but also the dataflow graphs of all equivalent functions reachable via a set of rewrite rules. In doing so, it can recognize an operation even if it uses alternate APIs, is in a different but mathematically-equivalent form, is split apart with temporary variables, or is interleaved with other code. Furthermore, it can recognize when code is an instance of some higher-level concept such as iterating through a file. Because of this, from a single query, Yogo can find equivalent code in multiple languages. Our evaluation further shows the utility of Yogo beyond code search: encoding a buggy pattern as a Yogo query, we found a bug in Oracle’s Graal compiler which had been missed by a hand-written static analyzer designed for that exact kind of bug. Yogo is built on the Cubix multi-language infrastructure, and currently supports Java and Python.},
    booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
    pages = {1066–1082},
    numpages = {17},
    keywords = {code search, equational reasoning},
    location = {London, UK},
    series = {PLDI 2020}
}

@article{kopell2018-language-parametric-transformation,
    author = {Koppel, James and Premtoon, Varot and Solar-Lezama, Armando},
    title = {One tool, many languages: language-parametric transformation with incremental parametric syntax},
    year = {2018},
    issue_date = {November 2018},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {2},
    number = {OOPSLA},
    url = {https://doi.org/10.1145/3276492},
    doi = {10.1145/3276492},
    abstract = {We present a new approach for building source-to-source transformations that can run on multiple programming languages, based on a new way of representing programs called incremental parametric syntax. We implement this approach in Haskell in our Cubix system, and construct incremental parametric syntaxes for C, Java, JavaScript, Lua, and Python. We demonstrate a whole-program refactoring tool that runs on all of them, along with three smaller transformations that each run on several. Our evaluation shows that (1) once a transformation is written, little work is required to configure it for a new language (2) transformations built this way output readable code which preserve the structure of the original, according to participants in our human study, and (3) our transformations can still handle language corner-cases, as validated on compiler test suites.},
    journal = {Proc. ACM Program. Lang.},
    month = oct,
    articleno = {122},
    numpages = {28},
    keywords = {refactoring, program transformation, expression problem}
}

@ARTICLE{silva2021-refdiff,
    author={Silva, Danilo and da Silva, João Paulo and Santos, Gustavo and Terra, Ricardo and Valente, Marco Tulio},
    journal={IEEE Transactions on Software Engineering}, 
    title={RefDiff 2.0: A Multi-Language Refactoring Detection Tool}, 
    year={2021},
    volume={47},
    number={12},
    pages={2786-2802},
    abstract={Identifying refactoring operations in source code changes is valuable to understand software evolution. Therefore, several tools have been proposed to automatically detect refactorings applied in a system by comparing source code between revisions. The availability of such infrastructure has enabled researchers to study refactoring practice in large scale, leading to important advances on refactoring knowledge. However, although a plethora of programming languages are used in practice, the vast majority of existing studies are restricted to the Java language due to limitations of the underlying tools. This fact poses an important threat to external validity. Thus, to overcome such limitation, in this paper we propose RefDiff 2.0, a multi-language refactoring detection tool. Our approach leverages techniques proposed in our previous work and introduces a novel refactoring detection algorithm that relies on the Code Structure Tree (CST), a simple yet powerful representation of the source code that abstracts away the specificities of particular programming languages. Despite its language-agnostic design, our evaluation shows that RefDiff's precision (96 percent) and recall (80 percent) are on par with state-of-the-art refactoring detection approaches specialized in the Java language. Our modular architecture also enables one to seamlessly extend RefDiff to support other languages via a plugin system. As a proof of this, we implemented plugins to support two other popular programming languages: JavaScript and C. Our evaluation in these languages reveals that precision and recall ranges from 88 to 91 percent. With these results, we envision RefDiff as a viable alternative for breaking the single-language barrier in refactoring research and in practical applications of refactoring detection.},
    keywords={Codes;Java;Source coding;History;Crawlers;Measurement;Refactoring;source code analysis;code repositories;git},
    doi={10.1109/TSE.2020.2968072},
    ISSN={1939-3520},
    month={Dec}
}

@inproceedings{vanTonder2019-syntax-transformation-ppc,
    author = {van Tonder, Rijnard and Le Goues, Claire},
    title = {Lightweight multi-language syntax transformation with parser parser combinators},
    year = {2019},
    isbn = {9781450367127},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3314221.3314589},
    doi = {10.1145/3314221.3314589},
    abstract = {Automatically transforming programs is hard, yet critical for automated program refactoring, rewriting, and repair. Multi-language syntax transformation is especially hard due to heterogeneous representations in syntax, parse trees, and abstract syntax trees (ASTs). Our insight is that the problem can be decomposed such that (1) a common grammar expresses the central context-free language (CFL) properties shared by many contemporary languages and (2) open extension points in the grammar allow customizing syntax (e.g., for balanced delimiters) and hooks in smaller parsers to handle language-specific syntax (e.g., for comments). Our key contribution operationalizes this decomposition using a Parser Parser combinator (PPC), a mechanism that generates parsers for matching syntactic fragments in source code by parsing declarative user-supplied templates. This allows our approach to detach from translating input programs to any particular abstract syntax tree representation, and lifts syntax rewriting to a modularly-defined parsing problem. A notable effect is that we skirt the complexity and burden of defining additional translation layers between concrete user input templates and an underlying abstract syntax representation. We demonstrate that these ideas admit efficient and declarative rewrite templates across 12 languages, and validate effectiveness of our approach by producing correct and desirable lightweight transformations on popular real-world projects (over 50 syntactic changes produced by our approach have been merged into 40+). Our declarative rewrite patterns require an order of magnitude less code compared to analog implementations in existing, language-specific tools.},
    booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
    pages = {363–378},
    numpages = {16},
    keywords = {parsers, rewriting, syntax, transformation},
    location = {Phoenix, AZ, USA},
    series = {PLDI 2019}
}

@inproceedings{ford2004-peg-inproceedings,
    author = {Ford, Bryan},
    title = {Parsing expression grammars: a recognition-based syntactic foundation},
    year = {2004},
    isbn = {158113729X},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/964001.964011},
    doi = {10.1145/964001.964011},
    abstract = {For decades we have been using Chomsky's generative system of grammars, particularly context-free grammars (CFGs) and regular expressions (REs), to express the syntax of programming languages and protocols. The power of generative grammars to express ambiguity is crucial to their original purpose of modelling natural languages, but this very power makes it unnecessarily difficult both to express and to parse machine-oriented languages using CFGs. Parsing Expression Grammars (PEGs) provide an alternative, recognition-based formal foundation for describing machine-oriented syntax, which solves the ambiguity problem by not introducing ambiguity in the first place. Where CFGs express nondeterministic choice between alternatives, PEGs instead use prioritized choice. PEGs address frequently felt expressiveness limitations of CFGs and REs, simplifying syntax definitions and making it unnecessary to separate their lexical and hierarchical components. A linear-time parser can be built for any PEG, avoiding both the complexity and fickleness of LR parsers and the inefficiency of generalized CFG parsing. While PEGs provide a rich set of operators for constructing grammars, they are reducible to two minimal recognition schemas developed around 1970, TS/TDPL and gTS/GTDPL, which are here proven equivalent in effective recognition power.},
    booktitle = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
    pages = {111–122},
    numpages = {12},
    keywords = {unified grammars, syntactic predicates, scannerless parsing, regular expressions, parsing expression grammars, packrat parsing, lexical analysis, context-free grammars, TDPL, GTDPL, BNF},
    location = {Venice, Italy},
    series = {POPL '04}
}

@article{ford2004-peg,
    author = {Ford, Bryan},
    title = {Parsing expression grammars: a recognition-based syntactic foundation},
    year = {2004},
    issue_date = {January 2004},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {39},
    number = {1},
    issn = {0362-1340},
    url = {https://doi.org/10.1145/982962.964011},
    doi = {10.1145/982962.964011},
    abstract = {For decades we have been using Chomsky's generative system of grammars, particularly context-free grammars (CFGs) and regular expressions (REs), to express the syntax of programming languages and protocols. The power of generative grammars to express ambiguity is crucial to their original purpose of modelling natural languages, but this very power makes it unnecessarily difficult both to express and to parse machine-oriented languages using CFGs. Parsing Expression Grammars (PEGs) provide an alternative, recognition-based formal foundation for describing machine-oriented syntax, which solves the ambiguity problem by not introducing ambiguity in the first place. Where CFGs express nondeterministic choice between alternatives, PEGs instead use prioritized choice. PEGs address frequently felt expressiveness limitations of CFGs and REs, simplifying syntax definitions and making it unnecessary to separate their lexical and hierarchical components. A linear-time parser can be built for any PEG, avoiding both the complexity and fickleness of LR parsers and the inefficiency of generalized CFG parsing. While PEGs provide a rich set of operators for constructing grammars, they are reducible to two minimal recognition schemas developed around 1970, TS/TDPL and gTS/GTDPL, which are here proven equivalent in effective recognition power.},
    journal = {SIGPLAN Not.},
    month = jan,
    pages = {111–122},
    numpages = {12},
    keywords = {unified grammars, syntactic predicates, scannerless parsing, regular expressions, parsing expression grammars, packrat parsing, lexical analysis, context-free grammars, TDPL, GTDPL, BNF}
}

@article{ierusalimschy2009-lpeg,
    author = {Ierusalimschy, Roberto},
    title = {A text pattern-matching tool based on Parsing Expression Grammars},
    year = {2009},
    issue_date = {March 2009},
    publisher = {John Wiley \& Sons, Inc.},
    address = {USA},
    volume = {39},
    number = {3},
    issn = {0038-0644},
    abstract = {Current text pattern-matching tools are based on regular expressions. However, pure regular expressions have proven too weak a formalism for the task: many interesting patterns either are difficult to describe or cannot be described by regular expressions. Moreover, the inherent non-determinism of regular expressions does not fit the need to capture specific parts of a match. Motivated by these reasons, most scripting languages nowadays use pattern-matching tools that extend the original regular-expression formalism with a set of ad hoc features, such as greedy repetitions, lazy repetitions, possessive repetitions, ‘longest-match rule,’ lookahead, etc. These ad hoc extensions bring their own set of problems, such as lack of a formal foundation and complex implementations. In this paper, we propose the use of Parsing Expression Grammars (PEGs) as a basis for pattern matching. Following this proposal, we present LPEG, a pattern-matching tool based on PEGs for the Lua scripting language. LPEG unifies the ease of use of pattern-matching tools with the full expressive power of PEGs. Because of this expressive power, it can avoid the myriad of ad hoc constructions present in several current pattern-matching tools. We also present a Parsing Machine that allows a small and efficient implementation of PEGs for pattern matching. Copyright © 2008 John Wiley \& Sons, Ltd.},
    journal = {Softw. Pract. Exper.},
    month = mar,
    pages = {221–258},
    numpages = {38},
    keywords = {scripting languages, pattern matching, Parsing Expression Grammars}
}

@article{lammel2003-syp,
    author = {L\"{a}mmel, Ralf and Jones, Simon Peyton},
    title = {Scrap your boilerplate: a practical design pattern for generic programming},
    year = {2003},
    issue_date = {March 2003},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {38},
    number = {3},
    issn = {0362-1340},
    url = {https://doi.org/10.1145/640136.604179},
    doi = {10.1145/640136.604179},
    abstract = {We describe a design pattern for writing programs that traverse data structures built from rich mutually-recursive data types. Such programs often have a great deal of "boilerplate" code that simply walks the structure, hiding a small amount of "real" code that constitutes the reason for the traversal.Our technique allows most of this boilerplate to be written once and for all, or even generated mechanically, leaving the programmer free to concentrate on the important part of the algorithm. These generic programs are much more adaptive when faced with data structure evolution because they contain many fewer lines of type-specific code.Our approach is simple to understand, reasonably efficient, and it handles all the data types found in conventional functional programming languages. It makes essential use of rank-2 polymorphism, an extension found in some implementations of Haskell. Further it relies on a simple type-safe cast operator.},
    journal = {SIGPLAN Not.},
    month = jan,
    pages = {26–37},
    numpages = {12},
    keywords = {generic programming, rank-2 types, traversal, type cast}
}

@article{matute2024-sequence-tree-matching,
    author = {Matute, Gabriel and Ni, Wode and Barik, Titus and Cheung, Alvin and Chasins, Sarah E.},
    title = {Syntactic Code Search with Sequence-to-Tree Matching: Supporting Syntactic Search with Incomplete Code Fragments},
    year = {2024},
    issue_date = {June 2024},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {8},
    number = {PLDI},
    url = {https://doi.org/10.1145/3656460},
    doi = {10.1145/3656460},
    abstract = {Lightweight syntactic analysis tools like Semgrep and Comby leverage the tree structure of code, making them more expressive than string and regex search. Unlike traditional language frameworks (e.g., ESLint) that analyze codebases via explicit syntax tree manipulations, these tools use query languages that closely resemble the source language. However, state-of-the-art matching techniques for these tools require queries to be complete and parsable snippets, which makes in-progress query specifications useless. We propose a new search architecture that relies only on tokenizing (not parsing) a query. We introduce a novel language and matching algorithm to support tree-aware wildcards on this architecture by building on tree automata. We also present stsearch, a syntactic search tool leveraging our approach. In contrast to past work, our approach supports syntactic search even for previously unparsable queries. We show empirically that stsearch can support all tokenizable queries, while still providing results comparable to Semgrep for existing queries. Our work offers evidence that lightweight syntactic code search can accept in-progress specifications, potentially improving support for interactive settings.},
    journal = {Proc. ACM Program. Lang.},
    month = jun,
    articleno = {230},
    numpages = {22},
    keywords = {Code Search, Syntactic Analysis, Tree Wildcards}
}

@article{baars2004-parsing-permutation-phrases,
    title={FUNCTIONAL PEARL Parsing permutation phrases},
    volume={14},
    DOI={10.1017/S0956796804005143},
    number={6},
    journal={Journal of Functional Programming},
    author={BAARS, ARTHUR I. and LÖH, ANDRES and SWIERSTRA, S. DOAITSE},
    year={2004},
    pages={635–646}
}

@article{adams2014-indentation-parsec,
    author = {Adams, Michael D. and A\u{g}acan, \"{O}mer S.},
    title = {Indentation-sensitive parsing for Parsec},
    year = {2014},
    issue_date = {December 2014},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {49},
    number = {12},
    issn = {0362-1340},
    url = {https://doi.org/10.1145/2775050.2633369},
    doi = {10.1145/2775050.2633369},
    abstract = {Several popular languages including Haskell and Python use the indentation and layout of code as an essential part of their syntax. In the past, implementations of these languages used ad hoc techniques to implement layout. Recent work has shown that a simple extension to context-free grammars can replace these ad hoc techniques and provide both formal foundations and efficient parsing algorithms for indentation sensitivity.However, that previous work is limited to bottom-up, LR($k$) parsing, and many combinator-based parsing frameworks including Parsec use top-down algorithms that are outside its scope. This paper remedies this by showing how to add indentation sensitivity to parsing frameworks like Parsec. It explores both the formal semantics of and efficient algorithms for indentation sensitivity. It derives a Parsec-based library for indentation-sensitive parsing and presents benchmarks on a real-world language that show its efficiency and practicality.},
    journal = {SIGPLAN Not.},
    month = sep,
    pages = {121–132},
    numpages = {12},
    keywords = {indentation sensitivity, layout, offside rule, parsec, parsing}
}

@book{pierce2002-types-and-programming-languages,
    author = {Pierce, Benjamin C.},
    title = {Types and Programming Languages},
    year = {2002},
    isbn = {0262162091},
    publisher = {The MIT Press},
    edition = {1st},
    address = {Cambridge, MA}
}

@article{henglein2011-regex-axiomatization,
    author = {Henglein, Fritz and Nielsen, Lasse},
    year = {2011},
    month = {January},
    pages = {385-398},
    title = {Regular Expression Containment: Coinductive Axiomatization and Computational Interpretation},
    volume = {46},
    journal = {Sigplan Notices - SIGPLAN},
    doi = {10.1145/1925844.1926429}
}

@techreport{plotkin1981-operational-semantics,
    title        = {A Structural Approach to Operational Semantics},
    author       = {Gordon D. Plotkin},
    institution  = {Computer Science Department, Aarhus University},
    year         = {1981},
    number       = {DAIMI FN-19}
}

@inproceedings{oliveira2015-avaliacao-automatica-programacao,
  author = {M{\'a}rcia de Oliveira and Elias Oliveira},
  title = {Abordagens, Pr{\'a}ticas e Desafios da Avalia{\c{c}}{\~a}o Autom{\'a}tica de Exerc{\'i}cios de Programa{\c{c}}{\~a}o},
  booktitle = {Anais do IV Workshop de Desafios da Computa{\c{c}}{\~a}o aplicada {\`a} Educa{\c{c}}{\~a}o},
  location = {Recife},
  year = {2015},
  keywords = {},
  issn = {0000-0000},
  pages = {131--140},
  publisher = {SBC},
  address = {Porto Alegre, RS, Brasil},
  doi = {10.5753/desafie.2015.10048},
  url = {https://sol.sbc.org.br/index.php/desafie/article/view/10048}
}