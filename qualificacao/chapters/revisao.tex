\chapter{Background}\label{chap:background}

This chapter presents fundamental concepts for understanding and development of the
work. Section~\ref{sec:operational-semantics} reviews concepts related to operational
semantics and Section~\ref{sec:type-systems} reviews concepts related to type systems.
Section~\ref{sec:pegs} formally introduces PEGs and their operational semantics.
Related work is discussed in Section~\ref{sec:related-work}.
Section~\ref{sec:background-conclusion} concludes the chapter.

The examples in this section will make use of Pierce's
\cite{pierce2002-types-and-programming-languages}
\(\mathbb{B}\mathbb{N}\) language for arithmetic expressions:
\[
    \begin{array}{lcl}
        t & \to & \texttt{true} \, \mid \, \texttt{false} \,
                \mid \, \texttt{if} \: t \: \texttt{then} \: t \: \texttt{else} \: t \,
                \mid \, \texttt{0} \, \mid \, \texttt{succ} \: t \,
                \mid \, \texttt{pred} \: t \, \mid \, \texttt{iszero} \: t
    \end{array}
\]
It features numbers, booleans, a test and a conditional expression.

\section{Operational semantics}\label{sec:operational-semantics}

In programming language theory, semantics is the rigorous mathematical study of
the meaning of programming languages. Semantics assign computational meaning to
valid strings in a programming language syntax. It describes the processes a
computer follows when executing a program in that specific language. There are
three basic approaches to formalizing semantics: operational semantics, denotational
semantics and axiomatic semantics.
In this work, we will focus on operational semantics.

Operational semantics specifies the behaviour of a programming language directly,
by defining a simple abstract machine. The meaning of a valid string is described
by the transitions it induces on states of the machine. Operational semantics
are classified in two categories: structural operational semantics and natural
semantics.

Structural operational semantics\cite{plotkin1981-operational-semantics} (also
called small-step semantics) defines the behaviour of a program in terms of the
behaviour of its parts, i.e., defines how the individual steps are performed.
Figure~\ref{fig:bn-operational-semantics} shows the small-step semantics for
\(\mathbb{B}\mathbb{N}\). Judgment \(t \to t'\) means expression \(t\) reduces
to \(t'\) in one step and \(to^\star\) represents the reflexive and transitive
closure of \(\to\). Example~\ref{fig:bn-small-step-example} presents the
evaluation of the term \texttt{if iszero 0 then false else true}.

\begin{figure*}[!h]
    \[
        \begin{array}{cc}
            \infer[_{\{E-IfTrue\}}]{\texttt{if}\:\texttt{true}\:\texttt{then}\:t_2\:\texttt{else}\:t_3 \to t_2}{}
            &
            \infer[_{\{E-IfFalse\}}]{\texttt{if}\:\texttt{false}\:\texttt{then}\:t_2\:\texttt{else}\:t_3 \to t_3}{}
            \\ \\
            \multicolumn{2}{c}{
                \infer[_{\{E-IfTrue\}}]{
                    \texttt{if}\:t_1\:\texttt{then}\:t_2\:\texttt{else}\:t_3
                    \to
                    \texttt{if}\:{t_1}'\:\texttt{then}\:t_2\:\texttt{else}\:t_3
                }{
                    t_1 \to {t_1}'
                }
            }
            \\ \\
            \infer[_{\{E-Succ\}}]{\texttt{succ}\:t_1 \to \texttt{succ}\:{t_1}'}{t_1 \to {t_1}'}
            &
            \infer[_{\{E-PredZero\}}]{\texttt{pred}\:0 \to 0}{}
            \\ \\
            \infer[_{\{E-PredSucc\}}]{\texttt{pred}\:\texttt{succ}\:t_1 \to t_1}{}
            &
            \infer[_{\{E-Pred\}}]{\texttt{pred}\:t_1 \to \texttt{pred}\:{t_1}'}{t_1 \to {t_1}'}
            \\ \\
            \infer[_{\{E-IszeroZero\}}]{\texttt{iszero}\:0 \to \texttt{true}}{}
            &
            \infer[_{\{E-IszeroSucc\}}]{\texttt{iszero}\:0 \to \texttt{false}}{}
            \\ \\
            \multicolumn{2}{c}{
                \infer[_{\{E-Iszero\}}]{\texttt{iszero}\:t_1 \to \texttt{iszero}\:{'t_1'}}{{t_1 \to {t_1}'}}
            }
        \end{array}
    \]
    \centering
    \caption{Structural operational semantics for \(\mathbb{B}\mathbb{N}\)}
    \label{fig:bn-operational-semantics}
\end{figure*}

\begin{example}
    \label{fig:bn-small-step-example}
    Small step evaluation of \texttt{if iszero 0 then false else true}

    \[
        \begin{array}{ll}
            \texttt{if iszero 0 then false else true} & \\
            \rightarrow\; \texttt{if true then false else true} & (\text{E-IszeroZero}) \\
            \rightarrow\; \texttt{false} & (\text{E-IfTrue})
        \end{array}
    \]
\end{example}

We say a term is in normal form when no evaluation rules applies to it.
When it is in normal form but not a value, i.e. we say it is stuck, and
so, it has no meaning according to the given semantics, such as
\(\texttt{succ}\:\texttt{true}\) or \(\texttt{pred}\:\texttt{false}\).

\section{Type systems}\label{sec:type-systems}

Since a term can get stuck at some stage, by reaching a term which no evaluation
rules applies, it corresponds to a meaningless or erroneous program. It would be
useful then, to be able to tell that a term will not get stuck during its evaluation
before actually evaluating such term. So, we need to be able to distinguish between
terms whose results will be a numeric value and terms whose result will be a boolean.
What we need, then, are types and a type system. We introduce two types, \texttt{Nat}
and \texttt{Bool}, for classifying terms this way. A type system is a set of rules on
how the terms of a language should be typed \cite{pierce2002-types-and-programming-languages}. 
Type checking is the process of verifying if the syntax has a valid derivation
on the type system, i.e., checking if the program has meaning.

Saying that a term \(t\) has type \(T\), means that \(t\) evaluates to a value
of the appropriate form. The typing relation, \(t : T\) for \(\mathbb{B}\mathbb{N}\)
is presented on Figure~\ref{fig:bn-typing-relation}.

\begin{figure*}[!h]
    \[
        \begin{array}{cc}
            \infer[_{\{T-True\}}]{\texttt{true} : \texttt{Bool}}{}
            &
            \infer[_{\{T-False\}}]{\texttt{false} : \texttt{Bool}}{}
            \\ \\
            \infer[_{\{T-Zero\}}]{0 : \texttt{Nat}}{}
            &
            \infer[_{\{T-Succ\}}]{\texttt{succ}\:t_1 : \texttt{Nat}}{t_1 : \texttt{Nat}}
            \\ \\
            \infer[_{\{T-Pred\}}]{\texttt{pred}\:t_1 : \texttt{Nat}}{t_1 : \texttt{Nat}}
            &
            \infer[_{\{T-IsZero\}}]{\texttt{iszero}\:t_1 : \texttt{Bool}}{t_1 : \texttt{Nat}}
            \\ \\
            \multicolumn{2}{c}{
                \infer[_{\{E-IfTrue\}}]{
                    \texttt{if}\:t_1\:\texttt{then}\:t_2\:\texttt{else}\:t_3 : T
                }{
                    t_1 : \texttt{Bool} & t_2 : T & t_3 : T
                }
            }
        \end{array}
    \]
    \centering
    \caption{Typing relation for \(\mathbb{B}\mathbb{N}\)}
    \label{fig:bn-typing-relation}
\end{figure*}

\begin{example}
    Type derivation for the term \texttt{if iszero 0 then false else true}.

    \[
        \infer[]{
            \texttt{if iszero 0 then false else true} : \texttt{Bool}
        }{
            \infer[]{
                \texttt{iszero}\:0 : \texttt{Bool}
            }{
                0 : \texttt{Nat}
            }
            &
            \infer[]{\texttt{false} : \texttt{Bool}}{}
            &
            \infer[]{\texttt{true} : \texttt{Bool}}{}
        }
    \]
\end{example}

When a term has a valid type derivation, we say it is well typed. Regardless of
meaning, we know it produces a value. No invalid terms such as \(\texttt{succ}\:
\texttt{true}\) or \(\texttt{pred}\:\texttt{false}\) will have a typing derivation,
but some terms that would not get stuck may not have a type derivation, such as
\texttt{if iszero 0 then 0 else true}, since the second and third arguments have 
different types. Type systems can help avoid several kinds of errors, both at 
compile and run time, but is inevitable to reject some programs that would result 
in a value.

\section{An Overview of PEGs}\label{sec:pegs}

Intuitively, PEGs are a formalism for describing top-down parsers.
Formally, a PEG is a 4-tuple \((V,\Sigma,R,e_S)\), where \(V\) is a
finite set of variables, \(\Sigma\) is the alphabet, \(R\) is the finite
set of rules, and \(e_S\) is the start expression. Each rule \(r \in R\) is a
pair \((A,e)\), usually written \(A \leftarrow e\), where \(A \in V\) and \(e\)
is a parsing expression. We let the meta-variable \(a\) denote an
arbitrary alphabet symbol, \(A\) a variable and \(e\) a parsing expression.
Following common practice, all meta-variables can appear primed or
sub-scripted. The following context-free grammar defines
the syntax of a parsing expression:
\[
   \begin{array}{lcl}
      e & \to & \epsilon \, \mid \, a \, \mid \, A\, \mid \,e_1\:e_2\,
                  \mid\,e_1\,/\,e_2\, \mid \,e^\star\, \mid \,!\,e \\
   \end{array}
\]
The execution of parsing expressions is defined by an inductively defined
judgment that relates pairs formed by a parsing expression and an input string
to pairs formed by the consumed prefix and the remaining string.
Notation \((e,s) \Rightarrow_G (s_p,s_r)\) denote that parsing expression \(e\)
consumes the prefix \(s_p\) from the input string \(s\) leaving the suffix \(s_r\).
The notation \((e,s) \Rightarrow_G \bot\) denote the fact that \(s\) cannot be
parsed by \(e\). We let meta-variable \(r\) denote an arbitrary parsing result, i.e.,
either \(r\) is a pair \((s_p,s_r)\) or \(\bot\). We say that an expression \(e\)
fails if its execution over an input produces \(\bot\); otherwise, it succeeds.
Figure~\ref{fig:pegsemantics} defines the PEG semantics.
\begin{figure}[H]
   \[
      \begin{array}{cc}
         \infer[_{\{Eps\}}]{(\epsilon,s) \Rightarrow_G (\epsilon,s)}{} &
         \infer[_{\{ChrS\}}]{(a,as_r) \Rightarrow_G (a,s_r)}{} \\ \\
         \infer[_{\{ChrF\}}]{(a,bs_r) \Rightarrow_G \bot}{a \neq b} &
         \infer[_{\{Var\}}]{(A,s) \Rightarrow_G (p, r)}
                        {A \leftarrow e \in R & (e,s) \Rightarrow_G (p,r) & s = pr} \\ \\
         \multicolumn{2}{c}{
            \infer[_{\{Cat_{S1}\}}]{(e_1\,e_2,s_1s_2s_r) \Rightarrow_G (\langle s_1,s_2 \rangle, s_r)}
                                 {(e_1,s_1s_2s_r) \Rightarrow_G (s_1,s_2s_r) &
                                 (e_2,s_2s_r)\Rightarrow_G (s_2,s_r)}
         } \\ \\
         \multicolumn{2}{c}{
            \infer[_{\{Cat_{F2}\}}]{(e_1\,e_2,s_ps_r) \Rightarrow_G \bot}
                                 { (e_1,s_ps_r) \Rightarrow_G (s_p,s_r) &
                                    (e_2,s_r) \Rightarrow_G \bot}} \\ \\
         \infer[_{\{Cat_{F1}\}}]{(e_1\,e_2,s)\Rightarrow_G \bot}{(e_1,s) \Rightarrow_G \bot} &
         \infer[_{\{Alt_{S1}\}}]{(e_1\,/\,e_2,s_p\,s_r) \Rightarrow_G (s_p,s_r)}
                                {(e_1,s_p\,s_r)\Rightarrow_G (s_p,s_r)} \\ \\
         \multicolumn{2}{c}{
           \infer[_{\{Alt_{S2}\}}]{(e_1\,/\,e_2,s_p\,s_r) \Rightarrow_G (s_p, s_r)}
                                  {(e_1,s_p\,s_r)\Rightarrow\ \bot &
                                   (e_2,s_p\,s_r)\Rightarrow_G (s_p,s_r)}
         } \\ \\
         \multicolumn{2}{c}{
            \infer[_{\{Star_{rec}\}}]{(e^\star,s_1s_2s_r) \Rightarrow_G (s_1\,s_2,s_r)}
                                 {(e,s_{p_1}s_{p_2}s_r) \Rightarrow_G (s_1,s_{p_2}s_r) &
                                  (e^\star, s_{p_2}s_r) \Rightarrow_G (s_2,s_r)}
         } \\ \\
         \infer[_{\{Star_{end}\}}]{(e^\star,s) \Rightarrow_G (\epsilon,s)}
                                    {(e,s) \Rightarrow_G \bot} &
         \infer[_{\{Not_F\}}]{(!\,e,s_p\,s_r) \Rightarrow_G \bot}
                          {(e,s_p\,s_r) \Rightarrow_G (s_p,s_r)}\\ \\
         \infer[_{\{Not_S\}}]{(!\,e,s) \Rightarrow_G (\epsilon,s)}
         {(e,s) \Rightarrow_G \bot}
           &
         \infer[_{\{ChrNil\}}]{(a,\epsilon) \Rightarrow_G \bot}{}
      \end{array}
   \]
   \centering
   \caption{Parsing expressions operational semantics that produces a tree.}
   \label{fig:pegsemantics}
\end{figure}

%
%
%
% \begin{figure*}[!ht]
%    \[
%       \begin{array}{cccc}
%          \infer[_{\{Eps\}}]{(\epsilon,s)\Rightarrow_G (\epsilon,s)}{} &
%          \infer[_{\{ChrS\}}]{(a,as_r) \Rightarrow_G (a,s_r)}{}  &
%          \infer[_{\{ChrF\}}]{(a,bs_r) \Rightarrow_G \bot}{a \neq b} &
%          \infer[_{\{Var\}}]{(A,s) \Rightarrow_G r}
%                            {A \leftarrow e \in R & (e,s) \Rightarrow_G r} \\ \\
%          \multicolumn{2}{c}{
%             \infer[_{\{Cat_{S1}\}}]{(e_1\,e_2,s_{p_1}s_{p_2}s_r) \Rightarrow_G (s_{p_1}s_{p_2}, s_r)}
%                                  {(e_1,s_{p_1}s_{p_2}s_r) \Rightarrow_G (s_{p_1},s_{p_2}s_r) &
%                                  (e_2,s_{p_2}s_r)\Rightarrow_G (s_{p_2},s_r)}
%          } &
%          \multicolumn{2}{c}{
%             \infer[_{\{Cat_{F2}\}}]{(e_1\,e_2,s_ps_r) \Rightarrow_G \bot}
%                                  { (e_1,s_ps_r) \Rightarrow_G (s_p,s_r) &
%                                     (e_2,s_r) \Rightarrow_G \bot}} \\ \\
%          \infer[_{\{Cat_{F1}\}}]{(e_1\,e_2,s)\Rightarrow_G \bot}{(e_1,s) \Rightarrow_G \bot} &
%          \infer[_{\{Alt_{S1}\}}]{(e_1\,/\,e_2,s_p\,s_r) \Rightarrow_G (s_p,s_r)}
%                                 {(e_1,s_p\,s_r)\Rightarrow_G (s_p,s_r)} &
%          \multicolumn{2}{c}{
%             \infer[_{\{Alt_{S2}\}}]{(e_1\,/\,e_2,s_p\,s_r) \Rightarrow_G r}
%                                   {(e_1,s_p\,s_r)\Rightarrow_G \bot &
%                                    (e_2,s_p\,s_r)\Rightarrow_G r}
%          } \\ \\
%          \multicolumn{2}{c}{
%             \infer[_{\{Star_{rec}\}}]{(e^\star,s_{p_1}s_{p_2}s_r) \Rightarrow_G (s_{p_1}s_{p_2},s_r)}
%                                  {(e,s_{p_1}s_{p_2}s_r) \Rightarrow_G (s_{p_1},s_{p_2}s_r) &
%                                   (e^\star, s_{p_2}s_r) \Rightarrow_G (s_{p_2},s_r)}
%          } &
%          \multicolumn{2}{c}{
%             \infer[_{\{Star_{end}\}}]{(e^\star,s) \Rightarrow_G (\epsilon,s)}
%                                     {(e,s) \Rightarrow_G \bot}} \\ \\
%          \multicolumn{2}{c}{
%             \infer[_{\{Not_F\}}]{(!\,e,s_p\,s_r) \Rightarrow_G \bot}
%                                {(e,s_p\,s_r) \Rightarrow_G (s_p,s_r)}
%          } &
%          \infer[_{\{Not_S\}}]{(!\,e,s) \Rightarrow_G (\epsilon,s)}
%          {(e,s) \Rightarrow_G \bot}
%            &
%          \infer[_{\{ChrNil\}}]{(a,\epsilon) \Rightarrow_G \bot}{}
%       \end{array}
%    \]
%    \centering
%    \caption{Parsing expressions operational semantics.}
%    \label{fig:pegsemantics}
% \end{figure*}

We comment on some rules of the semantics. Rule \(_{Eps}\) specifies that
expression \(\epsilon\) will not fail on any input \(s\) by leaving it unchanged.
Rule \(_{ChrS}\) specifies that an expression \(a\) consumes the first character when
the input string starts with an `a' and rule \(_{ChrF}\) shows that it fails when
the input starts with a different symbol. Rule \(_{Var}\) parses the input using
the expression associated with the variable in the grammar \(G\). When parsing a
sequence expression, \(e_1\:e_2\), the result is formed by \(e_1\) and \(e_2\) parsed
prefixes and the remaining input is given by \(e_2\). Rules \(_{Cat_{F1}}\) and
\(_{Cat_{F2}}\) say that if \(e_1\) or \(e_2\) fail, then the whole expression fails.
The rules for choice impose that we only try
expression \(e_2\) in \(e_1 / e_2\) when \(e_1\) fails. Parsing a star
expression \(e^\star\) consists in repeatedly execute \(e\) on the input string.
When \(e\) fails, \(e^\star\) succeeds without consuming any symbol of the input
string. Finally, the rules for the not predicate expression, \(!\,e\), specify
that whenever the expression \(e\) succeeds on input \(s\), \(!\,e\) fails; and when \(e\)
fails on \(s\) we have that \(!\,e\) succeeds without consuming any input.

\begin{example}\label{example:math-formulas-derivation}
   As an example, consider the PEG (Figure~\ref{fig:peg-arith-expressions}) which
   recognizes arithmetic expressions that apply the basic four operations to
   non-negative integers:

   \begin{figure*}[ht]
      \[
         \begin{array}{lcl}
            \text{E} & \leftarrow & \text{T ('+' T)}^\star \\
            \text{T} & \leftarrow & \text{F ('*' F)}^\star \\
            \text{F} & \leftarrow & \text{n / '(' E ')'} \\
         \end{array}
      \]
      \centering
      \caption{PEG for arithmetic expressions.}
      \label{fig:peg-arith-expressions}
   \end{figure*}

   Consider the string \texttt{1+2}. The initial rule \texttt{E}, first tries to
   parse a \texttt{T} that will, in turn, first try to consume a \texttt{F}, which
   recognizes the number \texttt{'1'}. The expression '+' TIt then consume the 
   \texttt{+} operator and tries to parse another \texttt{T}, which will consume 
   the 2 as a \texttt{F} and goes back to \texttt{E}, that does not find another 
   \texttt{'+'} operator and finalizes the parsing process. The derivation tree 
   for the string \texttt{1+2} can be seen on Figure~\ref{fig:peg-math-derivation}.

   \begin{sidewaysfigure*}
      \footnotesize
      \[
         \infer[]{
            \langle \text{E}, 1+2 \rangle \Rightarrow_G \langle 1+2, \epsilon \rangle
         }{
            \infer[]{
               \langle \text{T ('+' T)}^\star, 1+2 \rangle \Rightarrow_G \langle 1+2, \epsilon \rangle
            }{
               \infer[]{
                  \langle \text{T}, 1+2 \rangle \Rightarrow_G \langle 1, +2 \rangle
               }{
                  \infer[]{
                     \langle \text{F ('*' F)}^\star, 1+2 \rangle \Rightarrow_G \langle 1, +2 \rangle
                  }{
                     \infer[]{
                        \langle \text{F}, 1+2 \rangle
                     }{
                        \infer[]{
                           \langle \text{n / '(' E ')'}, 1+2 \rangle \Rightarrow_G \langle 1, +2 \rangle
                        }{
                           \infer[]{
                              \langle \text{n}, 1+2 \rangle \Rightarrow_G \langle 1, +2 \rangle
                           }{}
                        }
                     }
                     &
                     \infer[]{
                        \langle \text{('*' F)}^\star, +2 \rangle \Rightarrow_G \langle \epsilon, +2 \rangle
                     }{
                        \infer[]{
                           \langle \text{'*' F}, +2 \rangle \Rightarrow_G \langle \bot, +2 \rangle
                        }{}
                     }
                  }
               }
               &
               \infer[]{
                  \langle \text{('+' T)}^\star, +2 \rangle \Rightarrow_G \langle +2, \epsilon \rangle
               }{
                  \infer[]{
                     \langle \text{'+' T}, +2\rangle \Rightarrow_G \langle +2, \epsilon \rangle
                  }{
                     \infer[]{
                        \langle \text{'+'}, +2 \rangle \Rightarrow_G \langle +, 2 \rangle
                     }{}
                     &
                     \infer[]{
                        \langle \text{T}, 2 \rangle \Rightarrow_G \langle 2, \epsilon \rangle
                     }{
                        \infer[]{
                           \langle \text{F ('*' F)}^\star, 2 \rangle \Rightarrow_G \langle 2, \epsilon \rangle
                        }{
                           \infer[]{
                              \langle \text{F}, 2 \rangle \Rightarrow_G \langle 2, \epsilon \rangle
                           }{
                              \infer[]{
                                 \langle \text{n / '(' E ')'}, 2 \rangle \Rightarrow_G \langle 2, \epsilon \rangle
                              }{
                                 \infer[]{
                                    \langle \text{n}, 2 \rangle \Rightarrow_G \langle 2, \epsilon \rangle
                                 }{}
                              }
                           }
                           &
                           \infer[]{
                              \langle \text{('*' F)}^\star, \epsilon \rangle \Rightarrow_G \langle \epsilon, \epsilon \rangle
                           }{
                              \infer[]{
                                 \langle \text{'*' F}, \epsilon \rangle \Rightarrow_G \langle \bot, \epsilon \rangle
                              }{}
                           }
                        }
                     }
                  }
                  &
                  \infer[]{
                     \langle (\text{'+' T})^\star, \epsilon \rangle \Rightarrow_G \langle \epsilon, \epsilon \rangle
                  }{
                     \infer[]{
                        \langle \text{'+' T}, \epsilon \rangle \Rightarrow_G \langle \bot, \epsilon \rangle
                     }{}
                  }
               }
            }
         }
      \]
      \centering
      \caption{Semantic derivation for expression `1+2*3'}
      \label{fig:peg-math-derivation}
   \end{sidewaysfigure*}
\end{example}

\section{Related work}\label{sec:related-work}

Atkinson and Griswold~\cite{atkinson2006-effective-pattern-matching} presents
the matching tool TAWK, which extends extend the pattern syntax of AWK to
support matching of abstract syntax trees. In TAWK, pattern syntax is
language-independent, based on abstract tree patterns, and each pattern can
have associated actions, which are written in C for generality, familirity
and performance. An example of extracting a call graph from a given code is
presented throughout the paper, providing examples in different pattern matching
tools, including TAWK, and a scorecard for each, detailing four characteristics:
matching power, programming power, speed and robustness.

Kopell et al.~\cite{kopell2018-language-parametric-transformation} presents an
approach for building source-to-source transformation that can run on multiple
programming languages, based on a representation called incremental parametric
syntax (IPS).
In IPS, languages are represented using a mixture of language-specific and
generic parts. Transformations deal only with the generic fragments, but
the implementer starts with a pre-existing normal syntax definition, and
only does enough up-front work to redefine a small fraction of a language
in terms of these generic fragments.
The IPS was implemented in a Haskell framework called \textit{Cubix},
and currently supports C, Java, JavaScript, Lua, and Python\footnote{
See \url{https://github.com/cubix-framework/cubix}}.
They also demonstrate a whole-program refactoring for threading variables
through chains of function calls and three smaller source-to-source
transformations, being a hoisting transformation, a test-coverage
transformation and a the three-address code transformation.

Premtoon et al.~\cite{premtoon2020-code-search-equational-reasoning} presents
\textit{Yogo}, a tool that uses an approach to semantic code search
based on equational reasoning, that considers not only the dataflow graph of
a function, but also the dataflow graphs of all equivalents functions reachable
via a set of rewrite rules~\cite{premtoon2020-code-search-equational-reasoning}.
The tool is capable of recognizing differents variations of the same operation
and also when code is an instance of a higher-level concept.
\textit{Yogo} is built on the \textit{Cubix} infraestructure and can find
equivalent code in multiple languages from a single query.

Silva et al.~\cite{silva2021-refdiff} proposes \textit{RefDiff 2.0}, a
multi-language refactoring detection tool. Their approach introduces a
refactoring detection algorithm that relies on the Code Structure Tree
(CST), a representation of the source code that abstract away the
specificities of particular programming languages.
The tool has results that are on par with state-of-the-art refactoring
detection approaches specialized in the Java language. It also has support
for two other popular programming languages: JavaScript and C. This
demonstrates that the tool can be a viable alternative for multi-language
refactoring research and in practical applications of refactoring detection.

van Tonder and Le Goues~\cite{vanTonder2019-syntax-transformation-ppc} proposes
that the problem of automatically transforming programs can be decomposed by
having a common grammar and open extension points. The common grammar expresses
the central context-free language (CFL) properties shared by many contemporary
languages, while the open extensions points allow customizing syntax and hooks
in smaller parsers to handle language-specific syntax, such as comments.
The decomposition is made using a Parser Parser combinator (PPC), a mechanism
that generates parsers for matching syntactic fragments in source code by parsing
declarative user-supplied templates.
This allows to detach from translating input programs to any particular
abstract syntax tree representation, and lifts syntax rewriting to a
modularly-defined parsing problem.
They also evaluate \textit{Comby}, an implementation of the approach process
using PPC, on a large scale multi-language rewriting across 12 languages, and
validated effectiveness of the approach by producing correct and desirable
lightweight transformations on popular real-world projects.

Matute et al.~\cite{matute2024-sequence-tree-matching} proposes a search
architecture that relies only on tokenizing a query, introducing a new
language and matching algorithm to support tree-aware wildcards by building
on tree automata. They also present \textit{stsearch}, a syntactic search
tool leveraging their approach, which supports syntactic search even for
previously unparsable queries.

Ierusalimschy~\cite{ierusalimschy2009-lpeg} proposes the use of PEGs as a basis
for text pattern-mathing and presents LPEG, a text pattern-matching tool based on
PEGs for the Lua scripting language, and a Parsing Machine that allows a small
and efficient implementation of PEGs for pattern matching.
This allow LPEG to have both the expressive power of PEGs with the ease of use
of regular expressions.
LPEG also seems specially suited for languages that are too complex for
traditional pattern-matching tools but do not need a complex yacc-lex
implementation, like domain-specific languages such as SQL and regular
expressions, and even XML.

Of the approaches found in the literature, only LPEG uses the concept of PEGs,
trying to combine their expressive capacity with the ease of use of regular
expressions, but it focuses on text matching. The approach presented in this
work uses PEGs to generate a parse tree and perform the matching (and rewriting)
of these trees.

\section{Conclusion}\label{sec:background-conclusion}

This chapter presented the theoretical framework for this work: initially, concepts
about type systems and operational semantics were reviewed. Next, the concept
of PEGs and their operational semantics were presented. Finally, works related
to source code analysis and pattern matching were presented.

\cleardoublepage
