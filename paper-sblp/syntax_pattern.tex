% Configurações tiradas de https://cbsoft.sbc.org.br/2025/sblp/ 
\documentclass[10pt,sigplan,screen,review,anonymous]{acmart}
\setcopyright{none}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}

\usepackage{amsmath}
\usepackage{proof}
\usepackage{amsfonts}
\usepackage{enumitem}

\title{Title}
\author{Guilherme Augusto Anício Drummond do Nascimento \\ Universidade Federal de Ouro Preto}
\date{\today}

\begin{document}

\begin{abstract}
    Abstract
\end{abstract}

\maketitle

% \include{chapters/introduction}
\section{Introduction}
Automatic code analysis tools are commonly used in educational settings to
support the evaluation of programming assignments. While many existing tools
focus on correctness or performance, they often overlook the importance of
how students solve the given problems, particularly whether they follow the
concepts and constructs introduced during the course. 
In introductory programming courses, it is important not only to assess 
whether a student's solution is correct, but also to ensure that the 
student is applying the concepts and techniques taught in class. 
Automatic code analysis can help detect when students rely on language 
features or external solutions that bypass the intended learning objectives.
To adress this need, we propose a multi-language code analysis tool based on 
Parsing Expression Grammars (PEGs), designed to detect the use of advanced 
or unauthorized constructs in students' code submissions. This tool aims to 
assist educators in enforcing pedagogical boundaries while maintaining 
flexibility across different programming languages.

\section{An Overview of PEGs}

Intuitively, PEGs are a formalism for describing top-down parsers.
Formally, a PEG is a 4-tuple $(V,\Sigma,R,e_S)$, where $V$ is a 
finite set of variables, $\Sigma$ is the alphabet, $R$ is the finite 
set of rules, and $e_s$ is the start expression. Each rule $r \in R$ is a 
pair $(A,e)$, usually written $A \leftarrow e$, where $A \in V$ and $e$ 
is a parsing expression. We let the meta-variable $a$ denote an 
arbitrary alphabet symbol, $A$ a variable and $e$ a parsing expression. 
Following common practice, all meta-variables can appear primed or 
sub-scripted. The following context-free grammar defines
the syntax of a parsing expression:
\[
\begin{array}{lcl}
e & \to  & \epsilon \,\mid\, a \,\mid\, A\,\mid\,e_1\:e_2\,
\mid\,e_1\,/\,e_2\,\mid\,e^\star\,\mid\,!\,e\\
\end{array}
\]
The execution of parsing expressions is defined by an inductively defined
judgment that relates pairs formed by a parsing expression and an input string
to pairs formed by the consumed prefix and the remaining string.
Notation $(e,s) \Rightarrow_G (s_p,s_r)$ denote that parsing expression $e$
consumes the prefix $s_p$ from the input string $s$ leaving the suffix $s_r$.
The notation $(e,s) \Rightarrow_G \bot$ denote the fact that $s$ cannot be parsed by
$e$. We let meta-variable $r$ denote an arbitrary parsing result, i.e.,
either $r$ is a pair $(s_p,s_r)$ or $\bot$. We say that an expression $e$
fails if its execution over an input produces $\bot$; otherwise, it succeeds.
Figure~\ref{fig:pegsemantics} defines the PEG semantics.
\begin{figure*}[h!]
   \[
      \begin{array}{cccc}
         \infer[_{\{Eps\}}]{(\epsilon,s)\Rightarrow_G (\epsilon,s)}{} &
         \infer[_{\{ChrS\}}]{(a,as_r) \Rightarrow_G (a,s_r)}{}  &
         \infer[_{\{ChrF\}}]{(a,bs_r) \Rightarrow_G \bot}{a \neq b} &
         \infer[_{\{Var\}}]{(A,s) \Rightarrow_G r}
                           {A \leftarrow e \in R & (e,s) \Rightarrow_G r} \\ \\
         \multicolumn{2}{c}{
            \infer[_{\{Cat_{S1}\}}]{(e_1\,e_2,s_{p_1}s_{p_2}s_r) \Rightarrow_G (s_{p_1}s_{p_2}, s_r)}
                                 {(e_1,s_{p_1}s_{p_2}s_r) \Rightarrow_G (s_{p_1},s_{p_2}s_r) &
                                 (e_2,s_{p_2}s_r)\Rightarrow_G (s_{p_2},s_r)}
         } &
         \multicolumn{2}{c}{
            \infer[_{\{Cat_{F2}\}}]{(e_1\,e_2,s_ps_r) \Rightarrow_G \bot}
                                 { (e_1,s_ps_r) \Rightarrow_G (s_p,s_r) &
                                    (e_2,s_r) \Rightarrow_G \bot}} \\ \\
         \infer[_{\{Cat_{F1}\}}]{(e_1\,e_2,s)\Rightarrow_G \bot}{(e_1,s) \Rightarrow_G \bot} &
         \infer[_{\{Alt_{S1}\}}]{(e_1\,/\,e_2,s_p\,s_r) \Rightarrow_G (s_p,s_r)}
                                {(e_1,s_p\,s_r)\Rightarrow_G (s_p,s_r)} &
         \multicolumn{2}{c}{
            \infer[_{\{Alt_{S2}\}}]{(e_1\,/\,e_2,s_p\,s_r) \Rightarrow_G r}
                                  {(e_1,s_p\,s_r)\Rightarrow_G \bot &
                                   (e_2,s_p\,s_r)\Rightarrow_G r}
         } \\ \\
         \multicolumn{2}{c}{
            \infer[_{\{Star_{rec}\}}]{(e^\star,s_{p_1}s_{p_2}s_r) \Rightarrow_G (s_{p_1}s_{p_2},s_r)}
                                 {(e,s_{p_1}s_{p_2}s_r) \Rightarrow_G (s_{p_1},s_{p_2}s_r) &
                                  (e^\star, s_{p_2}s_r) \Rightarrow_G (s_{p_2},s_r)}
         } &
         \multicolumn{2}{c}{
            \infer[_{\{Star_{end}\}}]{(e^\star,s) \Rightarrow_G (\epsilon,s)}
                                    {(e,s) \Rightarrow_G \bot}} \\ \\
         \multicolumn{2}{c}{
            \infer[_{\{Not_F\}}]{(!\,e,s_p\,s_r) \Rightarrow_G \bot}
                               {(e,s_p\,s_r) \Rightarrow_G (s_p,s_r)}
         } &
         \infer[_{\{Not_S\}}]{(!\,e,s) \Rightarrow_G (\epsilon,s)}
         {(e,s) \Rightarrow_G \bot}
           &
         \infer[_{\{ChrNil\}}]{(a,\epsilon) \Rightarrow_G \bot}{}
      \end{array}
   \]
   \centering
   \caption{Parsing expressions operational semantics.}
   \label{fig:pegsemantics}
\end{figure*}
We comment on some rules of the semantics. Rule $_{Eps}$ specifies that
expression $\epsilon$ will not fail on any input $s$ by leaving it unchanged.
Rule $_{ChrS}$ specifies that an expression $a$ consumes the first character when
the input string starts with an `a' and rule $_{ChrF}$ shows that it fails when
the input starts with a different symbol. Rule $_{Var}$ parses the input using
the expression associated with the variable in the grammar $G$. When parsing a
sequence expression, $e_1\:e_2$, the result is formed by $e_1$ and $e_2$ parsed
prefixes and the remaining input is given by $e_2$. Rules $_{Cat_{F1}}$ and
$_{Cat_{F2}}$ say that if $e_1$ or $e_2$ fail, then the whole expression fails.
The rules for choice impose that we only try
expression $e_2$ in $e_1 / e_2$ when $e_1$ fails. Parsing a star
expression $e^\star$ consists in repeatedly execute $e$ on the input string.
When $e$ fails, $e^\star$ succeeds without consuming any symbol of the input
string. Finally, the rules for the not predicate expression, $!\,e$, specify
that whenever the expression $e$ succeeds on input $s$, $!\,e$ fails; and when $e$
fails on $s$ we have that $!\,e$ succeeds without consuming any input.



\section{Related work}

Atkinson and Griswold\cite{atkinson2006-effective-pattern-matching} presents 
the matching tool TAWK, which extends extend the pattern syntax of AWK to 
support matching of abstract syntax trees. In TAWK, pattern syntax is 
language-independent, based on abstract tree patterns, and each pattern can 
have associated actions, which are written in C for generality, familirity 
and performance. Throughout the paper, a prototypical example of extracting 
a call-graph from a given code, giving examples in differents tools for 
pattern matching.
At a later section, we also present an extraction of a call-graph using the
tool developed in this paper.

Kopell et al.\cite{kopell2018-language-parametric-transformation} presents an
approach for building source-to-source transformation that can run on multiple
programming languages, based on a representation called incremental parametric
syntax (IPS).
In IPS, languages are represented using a mixture of language-specific and 
generic parts. Transformations deal only with the generic fragments, but 
the implementer starts with a pre-existing normal syntax definition, and 
only does enough up-front work to redefine a small fraction of a language 
in terms of these generic fragments.
The IPS was implemented in a Haskell framework called \textit{Cubix}, 
and currentl supports C, Java, JavaScript, Lua, and Python.
They also demonstrate a whole-program refactoring for threading variables
through chains of function calls and three smaller source-to-source 
transformations, being a hoisting transformation, a test-coverage 
transformation and a the three-address code transformation.

Premtoon et al.\cite{premtoon2020-code-search-equational-reasoning} presents 
a tool called \textit{Yogo}, that uses an approach to semantic code search 
based on equational reasoning, that considers not only the dataflow graph of 
a function, but also the dataflow graphs of all equivalents functions reachable 
via a set of rewrite rules. The tool is capable of recognizing differents 
variations of the same operation and also when code is an instance of a 
higher-level concept.
\textit{Yogo} is built on the \textit{Cubix} multi-language infraestructure and can find
equivalent code in multiple languages from a single query.

Silva et al.\cite{silva2021-refdiff} proposes \textit{RefDiff 2.0}, a 
multi-language refactoring detection tool. Their approach introduces a 
refactoring detection algorithm that relies on the Code Structure Tree 
(CST), a representation of the source code that abstract away the 
specificities of particular programming languages. 
The tool has results that are on par with state-of-the-art refactoring 
detection approaches specialized in the Java language and has support 
for two other popular programming languages: JavaScript and C,
demonstrating that the tool can be a viable alternative for multi-language
refactoring research and in practical applications of refactoring detection.

van Tonder and Le Goues\cite{vanTonder2019-syntax-transformation-ppc} proposes
that the problem of automatically transforming programs can be decomposed such
that a common grammar expresses the central context-free language (CLF) 
properties shared by many contemporary languages and open extensions points
in the grammar allow customizing syntax and hooks in smaller parsers to handle
language-specific syntax, such as comments. The decomposition is made using a
Parser Parser combinator (PPC), a mechanism that generates parsers for matching
syntactic fragments in source code by parsing declarative user-supplied 
templates. 
This allows to detach from translating input programs to any particular 
abstract syntax tree representation, and lifts syntax rewriting to a 
modularly-defined parsing problem. 
They also evaluated \textit{Comby}, an implementation of the approach process using PPC,
on a large scale multi-language rewriting across 12 languages, and validated 
effectiveness of the approach by producing correct and desirable lightweight 
transformations on popular real-world projects.

Matute et al.\cite{matute2024-sequence-tree-matching} proposes a search
architecture that relies only on tokenizing a query, introducing a new 
language and matching algorithm to support tree-aware wildcards by building
on tree automata. They also present \textit{stsearch}, a syntactic search
tool leveraging their approach, which supports syntactic search even for
previously unparsable queries.

Ierusalimschy \cite{ierusalimschy2009-lpeg} proposes the use of PEGs as a basis
for text pattern-mathing and presents LPEG, a pattern-matching tool based on 
PEGs for the Lua scripting language, and a Parsing Machine that allows a small 
and efficient implementation of PEGs for pattern matching. 
This allow LPEG to have both the expressive power of PEGs with the ease of use 
of regular expressions.
LPEG also seems specially suited for languages that are too complex for 
traditional pattern-matching tools but do not need a complex yacc-lex 
implementation, like domain-specific languages such as SQL and regular
expressions, and even XML.

\section{References}
\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
